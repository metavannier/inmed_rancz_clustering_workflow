---
title: "Cluster analysis of neuronal properties."
subtitle: "Multivariate analyses."
author:
- Thomas Vannier
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
    html_document:
        theme:
            bootswatch: yeti
        toc: yes
        toc_float:
          collapsed: false
          smooth_scroll: true
        number_sections: yes
        df_print: kable
        code_folding: hide
---

<!-- Javascript for zooming on figures (adapted from: https://stackoverflow.com/questions/40401680) -->

<!-- Jquery import conflicts with DT::datatable so needs to be commented here -->
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script> -->

<style>
.zoomDiv {
  display: none;
  position: fixed;
  top: 50%;
  left: 50%;
  z-index: 50;
  transform: translate(-50%, -50%);
  background-color: #FFFFFF;
  box-shadow: 0px 0px 50px #888888;
  width: fit-content;
  max-width: 90%;
  max-height: 90%;
  overflow: auto;
}

.zoomImg {
  width: 150%;
}
</style>

<script type="text/javascript">
  $(document).ready(function() {
    $('body').prepend("<div class=\"zoomDiv\"><img src=\"\" class=\"zoomImg\"></div>");
    // onClick for all img except the zoomed one and link ones (filter)
    // use 'img.zoom' and out.extra='class=\"zoom\"' in chunk to specify manually which chunk images can be zoomed
    $('img:not(.zoomImg)').filter(':not(a *)').click(function() {
      $('.zoomImg').attr('src', $(this).attr('src'));
      $('.zoomDiv').show();
    })
    // onClick function for hiding div
    $('img.zoomImg').click(function() {
      $('.zoomDiv').hide();
    })
  })
</script>

This report present analyses to select the most relevant quantitative variables for clustering that would correspond to morphological classes.

```{r setup, warnings=F}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
```

```{r loadLibraries, echo=TRUE, warning=TRUE, message=TRUE}

# Loading library
library(ggplot2)
library(reshape2)
library(ggsignif)
library(caret)
library(rmarkdown)
library(tidyverse)
library(stringr)
library(tinytex)
library(knitr)
library(FactoMineR)
library(factoextra)
library(PCAmixdata)
library(gridExtra)
library(grid)
library(plotly)
library(ggrepel)
library(randomForest)
library(randomForestExplainer)
library(cluster)
library(dplyr)
library(mclust)
library(dendextend)
```

# Load the data

Summary of your data.

```{r loadData, echo=FALSE, warning=FALSE, message=TRUE}
# Load the 4 times merged dataset
DATA = snakemake@input[["data"]]
OUTPUT = snakemake@output[["multivariate_analyses_output"]]
TEST = snakemake@params[["test"]]

# Load your data
data <- read.csv(DATA, sep="\t", header=TRUE, row.names=1)

# Remove the second column
data <- data[,-1]

# Preserve the "Morphology" row
morphology_row <- data["Morphology", , drop = FALSE]
# Remove the row with the row name "Morphology"
data <- data[!rownames(data) %in% "Morphology", ]
# Convert all columns to numeric
data[] <- lapply(data, function(x) as.numeric(as.character(x)))
preProcess_missingdata <- preProcess(data, method = 'medianImpute')
data_imputed <- predict(preProcess_missingdata, data)

# Bind the "Morphology" row back with the imputed data
data_final <- rbind(morphology_row, data_imputed)

# Transpose the final data
t_data_final <- t(data_final)

# Convert the transposed data frame to ensure appropriate types
t_data_final <- as.data.frame(t_data_final, stringsAsFactors = FALSE)

# Convert "Morphology" column to factor
t_data_final$Morphology <- factor(t_data_final$Morphology)

# Convert all other columns to numeric
for (col in colnames(t_data_final)[colnames(t_data_final) != "Morphology"]) {
  t_data_final[, col] <- as.numeric(t_data_final[, col])
}

summary(t_data_final)

```

# Normality test

Perform a Kolmogorov-Smirnov test to check and print whether each numeric column in t_data_final follows a normal distribution (n > 50)

```{r normality, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

## Test the normality of the dependant variables
# Extract numeric columns (excluding "Morphology")
numeric_columns <- t_data_final[, colnames(t_data_final) != "Morphology"]

if (TEST == "shapiro") {

	# Perform Shapiro-Wilk test on each numeric column
	shapiro_results <- apply(numeric_columns, 2, shapiro.test)

	# Identify columns that follow a normal distribution (p > 0.05)
	normal_columns <- sapply(shapiro_results, function(x) x$p.value > 0.05)

	# Filter the numeric columns that follow a normal distribution
	numeric_columns_normal <- numeric_columns[, normal_columns]

	# Combine back with the "Morphology" column
	t_data_final_filtered <- cbind(Morphology = t_data_final[, "Morphology"], numeric_columns_normal)

	# Print the result
	cat("Variables that follow a normal distribution:\n")
	print(colnames(numeric_columns_normal))

} else if (TEST == "kolmogorov") {
	# Perform Kolmogorov-Smirnov test on each numeric column against a normal distribution
	ks_results <- apply(numeric_columns, 2, function(x) {
	ks.test(x, "pnorm", mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE))
	})

	# Identify columns that follow a normal distribution (p > 0.05)
	normal_columns <- sapply(ks_results, function(x) x$p.value > 0.05)

	# Filter the numeric columns that follow a normal distribution
	numeric_columns_normal <- numeric_columns[, normal_columns, drop = FALSE]

	# Combine back with the "Morphology" column
	t_data_final_filtered <- cbind(Morphology = t_data_final[, "Morphology"], numeric_columns_normal)

	# Print the result
	cat("Variables that follow a normal distribution based on Kolmogorov-Smirnov test:\n")
	print(colnames(numeric_columns_normal))
}

```

# Anova test
One-way Anova : testing differences between groups based on one independent variable (morphology).
```{r anova, echo=FALSE, warning=TRUE, message=TRUE, eval=TRUE}

# Extract column names for quantitative variables
quantitative_columns <- colnames(t_data_final_filtered)[-1]

# Initialize a list to store p-values
anova_pvalues <- list()

# Function to perform one-way ANOVA
perform_anova <- function(dependent_var, data, independent_var) {
  # Build the formula
  formula <- as.formula(paste(dependent_var, "~", independent_var))
  # Perform ANOVA
  anova_result <- aov(formula, data = data)
  # Extract the p-value
  p_value <- summary(anova_result)[[1]][["Pr(>F)"]][1]
  return(p_value)
}

# Iterate through each quantitative column, perform ANOVA, and store p-values
for (quant_col in quantitative_columns) {
  cat("\nANOVA Results for:", quant_col, " (p-value)\n")
  print(perform_anova(quant_col, t_data_final_filtered, colnames(t_data_final_filtered)[1]))
  p_value <- perform_anova(quant_col, t_data_final_filtered, colnames(t_data_final_filtered)[1])
  anova_pvalues[[quant_col]] <- p_value
}

# Filter columns with significant p-values (e.g., p < 0.05)
relevant_columns <- names(anova_pvalues)[unlist(anova_pvalues) < 0.05]

# Keep only relevant columns in the filtered dataset
t_data_final_relevant <- t_data_final_filtered[, c("Morphology", relevant_columns)]

# Print results
cat("Relevant columns for clustering:\n")
print(relevant_columns)

```

# Dimensionality Reduction with FAMD

Factor analysis of mixed data (FAMD) is a principal component method
dedicated to analyze a data set containing both quantitative and qualitative
variables. It makes it possible to analyze the similarity between
individuals by taking into account a mixed types of variables. Additionally, 
one can explore the association between all variables, both quantitative and 
qualitative variables.

We can first inspect the calculated principal dimensions (PDs), which are linear 
combinations of the original variables to better account for the variance in the dataset. 
Inspecting the eigenvalue and percentage variance explained by each PD, using scree plots, 
can provide insights into the “informativeness” of the original variables.

```{r famd, echo=FALSE, warning=TRUE, message=TRUE, eval=TRUE}
# Perform FAMD
res.famd <- FAMD(t_data_final, graph = FALSE)

## Visualize
a <- fviz_eig(res.famd,  
              choice='eigenvalue', 
              geom='line') 

b <- fviz_eig(res.famd) 
  
grid.arrange(a, b, ncol=2)

```

# Plot individual observations in new feature space

We can now visualize the individual data points in the 
new feature space created by the first three, and thus 
“most informative”, PDs. This is particularly useful when 
we want to see how “separable” groups of data points are, 
in our case in terms of morphology. To this end, the points are 
coloured by the variable “morphology”.

```{r ind_obs, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

## Concate original data with coordinates for 
## the first three principal dimensions
val_df <- as.data.frame(res.famd$ind)

x <- cbind(t_data_final, val_df[1:3])

## Plot
plot_ly(x, 
        x = ~coord.Dim.1, 
        y = ~coord.Dim.2, 
        z = ~coord.Dim.3, 
        color = ~Morphology) 

```

# Variable contribution

Contribution describes how much a variable accounts for the 
total variation captured by a given PD.

Top contributing variables to the first few PDs can provide 
insights into which variables underlie variations in the dataset, 
and may help with feature selection for downstream analyses. 
The red dashed line indicates the expected average contribution 
(100% contribution divided the total number of variables avaiable 
in the dataset). So variables meeting the cut-off would be considered 
as important in contributing to the PD.

```{r contribution, echo=FALSE, warning=TRUE, message=TRUE, eval=TRUE}

print("Visualization of the contribution of different modalities to the axis with the most amont of variance.")

# Contribution to the first dimension
fviz_contrib(res.famd, "var", axes = 1)
# Contribution to the second dimension
fviz_contrib(res.famd, "var", axes = 2)

```

# Correlation circle

As principal dimensions are linear combinations of the original variables,
understanding their relationships can help to identify
which variables are the most important in describing the 
total variance in a dataset.

The factor loading of a variable describes the correlation, i.e. information shared, 
between it and a given PD. By squaring the factor loading for a variable, we also get 
its squared loading (which you may see also called squared cosine or cos2). 
This provides a measure of the proportion of variance in a variable that is 
captured by a particular PD. For each variable, the sum of its squared loading 
across all PDs equals to 1.

```{r correlation, echo=FALSE, warning=TRUE, message=TRUE, eval=TRUE}

quanti.var <- get_famd_var(res.famd, "quanti.var")

fviz_famd_var(res.famd, "quanti.var", col.var = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

```

# Squared loading Plot

Squared loading plots allow us to visualize qualitative and quantitative 
variables together in the new feature space. 
The implementation provided by the PCAmixdata package has an 
added benefit of allowing the Morphology variable to be included as 
a supplementary variable, thereby seeing its relationship with 
other variables without including it in the original analysis. 
This is useful as most downstream analyses would try to predict Morphology.

```{r squared, echo=FALSE, warning=TRUE, message=TRUE, eval=TRUE}
## Split quantitative and qualitative variables
split <- splitmix(t_data_final)
res.pcamix <- PCAmix(X.quanti=split$X.quanti, X.quali=split$X.quali,  
                     graph=FALSE)

X1sup <- t_data_final[,1,drop=FALSE]
res.sup <- supvar(res.pcamix,X.quanti.sup = NULL, X.quali.sup = X1sup,rename.level=FALSE)

print(res.pcamix)
## Colour indiv obs by their squared loading
p <- fviz_famd_var(res.famd, 'var', 
                   axes = c(1, 2),
                   col.var = 'cos2')

par(mar = c(3,1,1,1))
fviz_add(p, res.sup$levels.sup$cos2,
         col.var = 'cos2')
par(xpd = TRUE)
```

# Varimax rotation

To further facilitate interpretation of the relationships between variables and PCs, 
additional rotation can be applied to PCs to result in high factor loadings for a 
few variables and low factor loadings for the rest. In other words, a small number 
of variables will become highly correlated with each PC. 

```{r varimax, echo=FALSE, warning=TRUE, message=TRUE, eval=FALSE}
## Apply varimax rotation to the first two PCs
res.pcarot <- PCArot(res.pcamix, dim=2,
                     graph=FALSE)

## Plot
par(mar = c(3,0.5,0.5,0.5))
plot(res.pcarot, choice="sqload",coloring.var=TRUE, axes=c(1, 2),srt=-20) 
par(xpd = TRUE)
```

# Random Forest

Random Forest is an ensemble learning method used for classification and regression tasks. 
It is based on the concept of Decision Trees but improves accuracy and reduces overfitting by 
creating multiple trees and aggregating their predictions.

I remove the variables not selected from the previous steps.

```{r RandomForest, echo=FALSE, warning=TRUE, message=TRUE, eval=TRUE}
# Remove the variables not selected in term of variable contribution, correlation circle and ANOVA
columns_to_remove <- c("tau", "AHP_time", "AP_peak_accomodation_absolute", 
                       "AP_width_adaptation", "firingrate_2xrheobase", 
                       "cfADP", "impedance_relative","AP_adaptation","fI_slope")
t_data_final_filtered <- t_data_final[, !colnames(t_data_final) %in% columns_to_remove]

# Make the training and test data
train <- t_data_final_filtered %>% sample_frac(0.8)
test<-anti_join(t_data_final_filtered, train)

set.seed(123) # permet de fixer les paramètres aléatoires de la rf
rf=randomForest(Morphology~ . , data = train,importance=T,ntree=500)
rf
print("We can then check if the default number of trees chosen (ntree=500) was sufficient.")
plot(rf)

varImpPlot(rf)

importance_frame <- measure_importance(rf)
print("Visualization of the variables that have the most impact on morphology in our dataset.")
plot_multi_way_importance(importance_frame, size_measure = "p_value")

# Model validation
print("Verification that the created model is relevant by attempting to predict the morphology of the cells in the test dataset.")
rf.results <- predict(rf,test)
results <- data.frame(actual = test$Morphology, prediction = rf.results)
results
# Calculate the number of correct predictions
correct_predictions <- sum(results$actual == results$prediction)
# Calculate the total number of predictions
total_predictions <- nrow(results)
# Calculate the percentage of correct predictions
accuracy_percentage <- (correct_predictions / total_predictions) * 100
# Print the result
print(paste0(accuracy_percentage, "% of good predictions"))


# We keep only the top variables
filtered_variables <- c("Morphology", as.character(importance_frame$variable[importance_frame$accuracy_decrease > 0.003]))
t_data_final_refiltered <- t_data_final[, colnames(t_data_final) %in% filtered_variables]

# Make the training and test data
train_2 <- t_data_final_refiltered %>% sample_frac(0.8)
test_2 <- anti_join(t_data_final_refiltered, train)

set.seed(123) # permet de fixer les paramètres aléatoires de la rf

rf_2=randomForest(Morphology~ . , data = train_2 ,importance=T,ntree=500)
rf_2
plot(rf_2)
varImpPlot(rf_2)

importance_frame_2 <- measure_importance(rf_2)
plot_multi_way_importance(importance_frame_2, size_measure = "p_value")


# Second model validation
print("Verification that the created model is relevant by attempting to predict the morphology of the cells in the test dataset.")
rf.results_2 <- predict(rf_2,test_2)
results_2 <- data.frame(actual = test_2$Morphology, prediction = rf.results_2)
results_2
# Calculate the number of correct predictions
correct_predictions <- sum(results_2$actual == results_2$prediction)
# Calculate the total number of predictions
total_predictions <- nrow(results_2)
# Calculate the percentage of correct predictions
accuracy_percentage <- (correct_predictions / total_predictions) * 100
# Print the result
print(paste0(accuracy_percentage, "% of good predictions"))

```

# Optimal Number of Clusters

The Elbow Method is a technique used to determine the optimal number of clusters in unsupervised clustering (e.g., k-means). 
It works by plotting the within-cluster sum of squares (WCSS) against the number of clusters. 
The "elbow" point, where the WCSS starts to decrease more slowly, indicates the optimal number of clusters. 
This point balances the trade-off between minimizing WCSS and avoiding overfitting.

```{r elbow, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}

data.scaled <- scale(t_data_final_refiltered[,-1])

### Elbow method (look at the knee)
# Elbow method for kmeans
fviz_nbclust(data.scaled, kmeans, method = "wss") + 
  geom_vline(xintercept = 4, linetype = 2)  # Check if 4 clusters (morphological classes) is optimal
```

The Silhouette Method evaluates the quality of clustering by measuring how similar an object is to its own cluster compared to other clusters. 
It calculates a silhouette score for each data point, ranging from -1 to 1. A score close to 1 indicates the object is well-matched to 
its cluster and poorly matched to others. The optimal number of clusters is chosen by maximizing the average silhouette score across all points.

```{r silhouette, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}
set.seed(123)
k <- 4  # Nombre de clusters
km_res <- kmeans(data.scaled, centers = k, nstart = 25)

sil_km <- silhouette(km_res$cluster, dist(data.scaled ))
fviz_silhouette(sil_km) 

# Average silhouette for kmeans
fviz_nbclust(data.scaled, kmeans, method = "silhouette")
```

# Unsupervised Clustering

Unsupervised Clustering using the selected parameters to explore relationships between morphology and physiology, as well as potential subclusters within the 4 morphological classes

Partitioning clustering with kmeans

```{r kmeans, echo=FALSE, warning=TRUE, message=TRUE, eval=TRUE}

#  1. Standardize the data (excluding the first column - morphology)
data_scaled <- scale(t_data_final_refiltered[,-1])

# 2. Perform k-means clustering (with k = 4)
km_res <- kmeans(data_scaled, centers = 4, nstart = 25)

# Save the original tab
data_for_kmeans <- t_data_final_refiltered

# 3. Add clustering results and morphology labels to the dataset
data_for_kmeans$cluster <- as.factor(km_res$cluster)  # Cluster assignment
data_for_kmeans$morphology <- as.factor(data_for_kmeans[, 1])  # Morphology info

# 4. Define color palette for morphology
color_palette <- c("#00AFBB", "#E7B800", "#FC4E07", "#2E9FDF")
names(color_palette) <- levels(data_for_kmeans$morphology)

# 5. Compute PCA for visualization
pca_res <- prcomp(data_scaled, scale = TRUE)  # Perform PCA

# 6. Convert PCA results into a dataframe for plotting
pca_data <- as.data.frame(pca_res$x)  # Extract PCA coordinates
pca_data$morphology <- data_for_kmeans$morphology  # Add morphology labels
pca_data$cluster <- data_for_kmeans$cluster  # Add k-means cluster labels

# 7. PCA Scatter Plot: Colors by Morphology, Ellipses by Cluster
ggplot(pca_data, aes(x = PC1, y = PC2, color = morphology)) +
  geom_point(size = 3) +  # Points are colored by morphology
  stat_ellipse(aes(group = cluster), linetype = "dashed", color = "black") +  # Dashed ellipses for clusters
  scale_color_manual(values = color_palette) +  # Apply morphology colors
  labs(title = "PCA of K-Means Clustering",
       color = "Morphology", 
       subtitle = "Ellipses show K-Means clusters") +
  theme_minimal() +  # Clean theme
  theme(legend.position = "right")  # Move legend to the right
```

Hierarchical clustering

```{r HierarchicalClustering, fig.width=12, fig.height=10, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE}
# List of clustering methods to iterate over
# clustering_methods <- c("ward.D", "ward.D2", "single", "complete", 
#                         "average", "mcquitty", "median", "centroid")

# clustering_methods <- c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski")

# Compute distance matrix
data_dist <- dist(t_data_final_refiltered[, -1], method = "canberra")

# Extract morphology information (first column)
morphology_labels <- as.factor(t_data_final_refiltered[, 1])

# Define color palette for morphology categories
color_palette <- c("#00AFBB", "#E7B800", "#FC4E07", "#2E9FDF")
names(color_palette) <- levels(morphology_labels)

num_cluster <- 4

# Perform hierarchical clustering with the current method
res <- hclust(data_dist, method = "ward.D2")
  
# Reorder morphology labels to match clustering order
ordered_labels <- morphology_labels[res$order]
  
# Convert reordered morphology labels to corresponding colors
label_colors <- color_palette[ordered_labels]
  
# Create a new plotting page to avoid overlap
grid.newpage()
  
# Adjust plot layout and margins
par(mfrow = c(1, 2), mar = c(4, 2, 2, 8))  # Increase margin for labels
  
# Plot dendrogram with black branches and colored labels
print(fviz_dend(res, k= num_cluster, rect = TRUE, rect_fill = TRUE, rect_border = "black", cex = 0.6,
                labels_track_height = 1.5,  
                label_cols = label_colors,
                k_colors = "black",
                main = "Dendrogram - Method: Ward.D2"))  # Add title
# Second plot: Empty plot for legend
plot.new()
legend("top", legend = levels(morphology_labels), fill = color_palette, 
       cex = 0.8, bty = "n", title = "Morphology Categories")
```


```{r sessioninfo}
t_data_final_refiltered
sessionInfo()
```

```{r Output}
# Create the output file for the snakemake rule

TEXT_OUTPUT <- snakemake@output[["multivariate_analyses_output"]]

output_file<-file(TEXT_OUTPUT)
writeLines(c("Rules multivariate analyses finished"), output_file)
close(output_file)
```
